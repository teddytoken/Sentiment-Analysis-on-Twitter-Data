{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMrwmJrYP2JHhF/kt8C+9Ht",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/teddytoken/Sentiment-Analysis-on-Twitter-Data/blob/master/TwitterSentimentAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7I09lWpTFTk2"
      },
      "outputs": [],
      "source": [
        "# utilities :\n",
        "import re # regular expression library\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cols = ['id', 'src', 'label', 'tweet']\n",
        "df_train = pd.read_csv(('/content/sample_data/twitter_training.csv'), names= cols)\n",
        "df_valid= pd.read_csv(('/content/sample_data/twitter_validation.csv'), names= cols)\n",
        "print(df_train.head())"
      ],
      "metadata": {
        "id": "AMa1dAKoFi1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_train.info())"
      ],
      "metadata": {
        "id": "w5GR6sWGFlwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.columns"
      ],
      "metadata": {
        "id": "WaNJRdBJFohw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.shape"
      ],
      "metadata": {
        "id": "AS36nzMOFrpz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for Null values :\n",
        "print(f'train nulls:', df_train.isna().sum())\n",
        "\n",
        "print(f'validation nulls:', df_valid.isna().sum())"
      ],
      "metadata": {
        "id": "6osL9mdwFuW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train= df_train.dropna(subset=['tweet'])"
      ],
      "metadata": {
        "id": "krNr20zgFwZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "sns.countplot(x='label', data=df_train, color='orange')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MPmQBmkvF1Oe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "plt.figure(figsize=(5, 5))\n",
        "wc = WordCloud(max_words=100, width=1600, height=800).generate(\" \".join(df_train['tweet']))\n",
        "plt.imshow(wc, interpolation='bilinear')\n",
        "plt.axis('off')  # Hide axes for better visualization\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Xc7Pm8XfGOd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "D-bnnja6GW6U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy as spc\n",
        "tokenizer = spc.load('en_core_web_sm')\n",
        "\n",
        "def preprocessing_pipe(texts):\n",
        "    docs = tokenizer.pipe(\n",
        "        texts,\n",
        "        n_process=4,  # No of CPUs\n",
        "        batch_size=64,\n",
        "    )\n",
        "    for doc in docs:\n",
        "        yield \" \".join([token.lemma_ for token in doc if not token.is_stop and not token.is_punct])"
      ],
      "metadata": {
        "id": "KDCIOXNOGTeT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vectorization"
      ],
      "metadata": {
        "id": "YqOyOftKGkRV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "def vectorization(tratxt, valtxt=None):\n",
        "    vect = TfidfVectorizer()\n",
        "    x_train = vect.fit_transform(tratxt)\n",
        "    if valtxt is not None:\n",
        "        x_val = vect.transform(valtxt)\n",
        "        return x_train, x_val, vect\n",
        "    return x_train, vect"
      ],
      "metadata": {
        "id": "7ecQAVZWGhj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_train.shape)\n",
        "print(df_valid.shape)"
      ],
      "metadata": {
        "id": "avgLHFsJGq41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoding"
      ],
      "metadata": {
        "id": "X8Jhl30MG3k2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "df_train['label'] = le.fit_transform(df_train['label'])\n",
        "df_valid['label'] = le.transform(df_valid['label'])"
      ],
      "metadata": {
        "id": "cL58XUpuG7K4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Applying preprocessing to each tweet in the Series\n",
        "tqdm.pandas()\n",
        "df_train['cleanedtweet'] = list(preprocessing_pipe(df_train['tweet']))\n",
        "df_valid['cleanedtweet'] = list(preprocessing_pipe(df_valid['tweet']))\n",
        "\n",
        "x, vect = vectorization(df_train['cleanedtweet'])\n",
        "y = df_train['label']\n",
        "\n",
        "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Applying preprocessing to each tweet in the Series\n",
        "x_test = vect.transform(df_valid['cleanedtweet'])\n",
        "y_test = df_valid['label']"
      ],
      "metadata": {
        "id": "UVbBaF5-INz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "model = Sequential(\n",
        "    [\n",
        "        Dense(64, activation='relu', input_dim=x_train.shape[1]),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dense(y_train.nunique(), activation='softmax')\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "uG9ScQlqIv4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "zK51BOS-JwCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x_train, y_train, epochs=10, validation_data=(x_val, y_val))"
      ],
      "metadata": {
        "id": "AYGq0m-bKcT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(x_test, y_test)"
      ],
      "metadata": {
        "id": "rp6eT0EYMDjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Evaluation"
      ],
      "metadata": {
        "id": "TbUKtjtlNiix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "y_preds = model.predict(x_test)\n",
        "y_pred_classes = np.argmax(y_preds, axis=1) # Convert probabilities to class labels\n",
        "print(classification_report(y_test, y_pred_classes))"
      ],
      "metadata": {
        "id": "ulEqQy1dMUdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(y_test, y_pred_classes)\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm,\n",
        "            annot=True,\n",
        "            fmt='d',\n",
        "            cmap='viridis',\n",
        "            xticklabels=le.classes_,  # Use the encoder's stored class names\n",
        "            yticklabels=le.classes_)  # Use the encoder's stored class names\n",
        "\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ORDIRU97MYLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TestingTxt = 'My Friend Pedro is the best'\n",
        "pre_txt = preprocessing_pipe([TestingTxt])\n",
        "vect_txt = vect.transform(pre_txt)\n",
        "pred = model.predict(vect_txt)"
      ],
      "metadata": {
        "id": "9TgfbH-ENU0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testingresult = le.inverse_transform([np.argmax(pred)])\n",
        "testingresult"
      ],
      "metadata": {
        "id": "pu9514vwNWLA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}